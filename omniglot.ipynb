{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Siamese Neural Network - omniglot dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input, backend\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Omniglot dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load('omniglot', split=['train','test'], with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = ds_info.features['image'].shape\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ds_info.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_img(left, right):\n",
    "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "  # left, right = case\n",
    "  if left[\"alphabet\"] == right[\"alphabet\"]:\n",
    "    flag = 0\n",
    "  else:\n",
    "    flag = 1\n",
    "  return tf.cast(left[\"image\"], tf.float32) / 255., tf.cast(right[\"image\"], tf.float32) / 255., tf.cast(flag, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(ds_info.splits['train'].num_examples / 2 * 0.8)\n",
    "# num_val = int(ds_info.splits['train'].num_examples / 2 * 0.2)\n",
    "left_train = ds_train.take(num_train)\n",
    "right_train = ds_train.skip(num_train).take(num_train)\n",
    "# left_val = ds_train.skip(num_train + num_train).take(num_val)\n",
    "# right_val = ds_train.skip(num_train + num_train + num_val).take(num_val)\n",
    "\n",
    "ds_train = tf.data.Dataset.zip((left_train, right_train))\n",
    "# ds_val = tf.data.Dataset.zip((left_val, right_val))\n",
    "\n",
    "train_x_left = []\n",
    "train_x_right = []\n",
    "train_y = []\n",
    "\n",
    "for left, right in ds_train:\n",
    "    left_x, right_x, flag = normalize_img(left, right)\n",
    "\n",
    "    train_x_left.append(left_x)\n",
    "    train_x_right.append(right_x)\n",
    "    train_y.append(flag)\n",
    "\n",
    "train_x_left = np.array(train_x_left)\n",
    "train_x_right = np.array(train_x_right)\n",
    "train_y = np.array(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_x_left = []\n",
    "# val_x_right = []\n",
    "# val_y = []\n",
    "\n",
    "# for left, right in ds_train:\n",
    "#     left_x, right_x, flag = normalize_img(left, right)\n",
    "\n",
    "#     val_x_left.append(left_x)\n",
    "#     val_x_right.append(right_x)\n",
    "#     val_y.append(flag)\n",
    "\n",
    "# val_x_left = np.array(val_x_left)\n",
    "# val_x_right = np.array(val_x_right)\n",
    "# val_y = np.array(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = int(ds_info.splits['test'].num_examples / 2)\n",
    "left_test = ds_test.take(num_test)\n",
    "right_test = ds_test.skip(num_test).take(num_test)\n",
    "ds_test = tf.data.Dataset.zip((left_test, right_test))\n",
    "\n",
    "test_x_left = []\n",
    "test_x_right = []\n",
    "test_y = []\n",
    "\n",
    "for left, right in ds_test:\n",
    "    left_x, right_x, flag = normalize_img(left, right)\n",
    "\n",
    "    test_x_left.append(left_x)\n",
    "    test_x_right.append(right_x)\n",
    "    test_y.append(flag)\n",
    "\n",
    "test_x_left = np.array(test_x_left)\n",
    "test_x_right = np.array(test_x_right)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x_left.shape, train_x_right.shape, train_y.shape)\n",
    "print(test_x_left.shape, test_x_right.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_img(case):\n",
    "#   \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "#   return tf.cast(case[\"image\"], tf.float32) / 255., case[\"alphabet\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_train = ds_train.map(\n",
    "#   normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# ds_train = ds_train.cache()\n",
    "# ds_train = ds_train.shuffle(int(ds_info.splits['train'].num_examples/2))\n",
    "# ds_train = ds_train.batch(128)\n",
    "# ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds_test = ds_test.map(\n",
    "#     normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# ds_test = ds_test.batch(128)\n",
    "# ds_test = ds_test.cache()\n",
    "# ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, margin = 1):\n",
    "    \"\"\"Implementation of the triplet loss function\n",
    "\n",
    "    Arguments:\n",
    "        y_true : true labels, required when you define a loss in Keras, \n",
    "                not applied in this function.\n",
    "\n",
    "        y_pred (list): python list containing three objects:\n",
    "            anchor : the encodings for the anchor data\n",
    "            positive : the encodings for the positive data (similar to anchor)\n",
    "            negative : the encodings for the negative data (different from anchor)\n",
    "        \n",
    "        margin (float, optional): m > 0 determines how far the embeddings of \n",
    "                    a negative data should be pushed apart. Defaults to 0.4.\n",
    "\n",
    "    Returns:\n",
    "        loss (float): real number, value of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    anchor = y_pred[0]\n",
    "    positive = y_pred[1]\n",
    "    negative = y_pred[2]\n",
    "\n",
    "    # squared distance between the anchor and the positive\n",
    "    pos_dist = tf.math.reduce_sum(tf.math.square(anchor - positive), axis=1)\n",
    "\n",
    "    # squared distance between the anchor and the negative\n",
    "    neg_dist = tf.math.reduce_sum(tf.math.square(anchor - negative), axis=1)\n",
    "\n",
    "    # compute loss\n",
    "    basic_loss = margin + pos_dist - neg_dist\n",
    "    loss = tf.math.maximum(basic_loss,0.0)\n",
    "    loss = tf.math.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def contrastive_loss(y_true, y_pred, margin = 1):\n",
    "    \"\"\"Implementation of the triplet loss function\n",
    "\n",
    "    Inspired by https://stackoverflow.com/questions/38260113/implementing-contrastive-loss-and-triplet-loss-in-tensorflow\n",
    "\n",
    "    Args:\n",
    "        y_true (int): true label, positive pair (same class) -> 0, \n",
    "                    negative pair (different class) -> 1\n",
    "        \n",
    "        y_pred (list): python list containing two objects in a pair of tensors:\n",
    "            left : the encodings for one image data in a pair\n",
    "            right : the encodings for the other image data in a pair\n",
    "        \n",
    "        margin (float, optional): m > 0 determines how far the embeddings of \n",
    "                    a negative pair should be pushed apart. Defaults to 0.4.\n",
    "\n",
    "    Returns:\n",
    "        loss (float): real number, value of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    # left = y_pred[0]\n",
    "    # right = y_pred[1]\n",
    "\n",
    "    # # squared distance between the left image and the right image\n",
    "    # dist = tf.math.reduce_sum(tf.math.square(left - right), 0)\n",
    "    dist = y_pred\n",
    "\n",
    "    loss_positive = tf.math.square(dist)\n",
    "    loss_negative = tf.math.square(tf.maximum(0., margin - dist))\n",
    "    \n",
    "    loss = y_true * loss_negative + (1 - y_true) * loss_positive\n",
    "    loss = 0.5 * tf.math.reduce_mean(loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/one-shot-learning-with-siamese-networks-using-keras-17f34e75bb3d\n",
    "\n",
    "# https://keras.io/examples/vision/siamese_contrastive/\n",
    "\n",
    "def euclidean_distance(vec):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vec: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vec\n",
    "    distance = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(x - y), axis=1, keepdims=True))\n",
    "    return distance\n",
    "\n",
    "\n",
    "\n",
    "# def siamese_networks(input_shape):\n",
    "\n",
    "#     input_layer = Input(shape=input_shape)\n",
    "#     x = layers.Conv2D(filters=64, kernel_size=(10,10), activation='relu')(input_layer)\n",
    "#     x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "#     x = layers.Conv2D(filters=128, kernel_size=(7,7), activation='relu')(x)\n",
    "#     x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "#     x = layers.Conv2D(filters=128, kernel_size=(4,4), activation='relu')(x)\n",
    "#     x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "#     x = layers.Conv2D(filters=256, kernel_size=(4,4), activation='relu')(x)\n",
    "#     x = layers.Flatten()(x)\n",
    "#     x = layers.Dense(4096, activation='sigmoid')(x)\n",
    "\n",
    "#     model = keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "#     input1 = Input(input_shape)\n",
    "#     input2 = Input(input_shape)\n",
    "\n",
    "#     left_model = model(input1)\n",
    "#     right_model = model(input2)\n",
    "#     L1_distance = layers.Lambda(euclidean_distance)([left_model, right_model])\n",
    "#     # L1_layer = layers.Lambda(lambda tensors:backend.abs(tensors[0] - tensors[1]))\n",
    "#     # L1_distance = L1_layer([left_model, right_model])\n",
    "\n",
    "#     prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "#     siamese_model = keras.Model(inputs=[input1, input2], outputs=prediction, name=\"siamese_networks\")\n",
    "\n",
    "#     return model, siamese_model\n",
    "\n",
    "def siamese_networks(input_shape):\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(10,10), activation='relu')(input_layer)\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(7,7), activation='relu')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(4,4), activation='relu')(x)\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(filters=256, kernel_size=(4,4), activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(4096, activation='sigmoid')(x)\n",
    "\n",
    "    model = keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    input1 = Input(input_shape)\n",
    "    input2 = Input(input_shape)\n",
    "    input3 = Input(input_shape)\n",
    "\n",
    "    left_model = model(input1)\n",
    "    right_model = model(input2)\n",
    "    L1_distance = layers.Lambda(euclidean_distance)([left_model, right_model])\n",
    "\n",
    "    prediction = layers.Dense(1,activation='sigmoid')(L1_distance)\n",
    "\n",
    "    siamese_model = keras.Model(inputs=[input1, input2], outputs=prediction, name=\"siamese_networks\")\n",
    "\n",
    "    return model, siamese_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, siamese_model = siamese_networks(input_shape)\n",
    "keras.utils.plot_model(model, \"sub-model.png\", show_shapes=True)\n",
    "keras.utils.plot_model(siamese_model, \"siamese_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A2_i = np.random.randint(0, 256, size=(1, 105, 105, 3)).astype(\"float32\")\n",
    "# A2_j = np.random.randint(0, 256, size=(1, 105, 105, 3)).astype(\"float32\")\n",
    "\n",
    "# model([A2_i, A2_j]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_model.compile(\n",
    "    loss=contrastive_loss,\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    "    metrics=['accuracy'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='accuracy', patience=2)\n",
    "\n",
    "# lr_reducer = keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                                 factor=0.5,\n",
    "#                                                 patience=5,\n",
    "#                                                 verbose=0,\n",
    "#                                                 mode='auto',\n",
    "#                                                 min_delta=0.0001,\n",
    "#                                                 cooldown=0,\n",
    "#                                                 min_lr=0)\n",
    "\n",
    "history = siamese_model.fit(\n",
    "    x=[train_x_left, train_x_right],\n",
    "    y=train_y,\n",
    "    epochs=10,\n",
    "    batch_size=128, #128\n",
    "    # validation_data=([val_x_left, val_x_right], val_y),\n",
    "    # callbacks=[early_stopping, lr_reducer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = siamese_model.evaluate(x=[test_x_left,test_x_right],y=test_y, verbose=1)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# target_names = ['0', '1']\n",
    "\n",
    "# print(classification_report(test_y, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # score = siamese_model_1.predict(x=[test_x_left,test_x_right], verbose=1)\n",
    "\n",
    "    # print('Test loss:', score[0])\n",
    "    # print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5fbb69dc477f16c614d0279d3c32a08b8462b181ba1381a242aa9ddc9865207"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
