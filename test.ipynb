{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow INFO and WARNING messages are not printed\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Input, backend, Model\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    \"\"\" - load dataset from tensorflow dataset\n",
    "        - print related information to console.\n",
    "        - split into training and test sets\n",
    "    Args:\n",
    "        dataset (str): name of the tensorflow dataset.\n",
    "\n",
    "    Returns:\n",
    "        ds_train (tf.data.Dataset): training set\n",
    "        ds_test (tf.data.Dataset): test set\n",
    "        ds_info (tfds.core.DatasetInfo): info about the dataset\n",
    "    \"\"\"\n",
    "    print('Start loading dataset - ' + dataset + '...\\n')\n",
    "    # split into training and test sets\n",
    "    (ds_train, ds_test), ds_info = tfds.load(dataset, split=['train','test'], with_info=True, shuffle_files=True)\n",
    "\n",
    "    print('Dataset  - ' + dataset + ' loaded into train and test splits successfully.\\n')\n",
    "\n",
    "    print(\"The list of all available labels for this dataset:\")\n",
    "    print(list(ds_info.features.keys())) # extract available labels from ds_info \n",
    "    print()\n",
    "\n",
    "    print(\"The input shape of the provided image in the dataset:\")\n",
    "    print(ds_info.features['image'].shape) # extract image shape from ds_info\n",
    "    print()\n",
    "\n",
    "    # print the size of training and test sets to console\n",
    "    print(\"The number of images in the training set: \" + str(ds_info.splits['train'].num_examples))\n",
    "    print(\"The number of images in the test set: \" + str(ds_info.splits['test'].num_examples))\n",
    "    print()\n",
    "\n",
    "    return ds_train, ds_test, ds_info\n",
    "\n",
    "\n",
    "def split_dataset_for_contrastive_networks(ds_train, ds_test, ds_info, label):\n",
    "    \"\"\"Split the training and test sets into subsets for inputing into contrastive networks\n",
    "\n",
    "    Steps: \n",
    "        1. split the training or test sets into two groups, left and right\n",
    "        2. make pairing of the left image and right image, and\n",
    "            check if two images belonging to same alphabet class by flag\n",
    "        3. store left images, right images and flag to three numpy array\n",
    "\n",
    "    Args:\n",
    "        dataset (dict): a dictionary store all the images and labels in the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def pairing(left, right):\n",
    "        \"\"\" - check if two images belonging to same alphabet class by label \n",
    "                same class: 0, not same: 1\n",
    "            - normalizes images: `uint8` -> `float32`.\n",
    "\n",
    "        Args:\n",
    "            left ([type]): [description]\n",
    "            right ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            the left image, right image and label in `float32`\n",
    "        \"\"\"\n",
    "        # same class: 0\n",
    "        if left[label] == right[label]:\n",
    "            flag = 0\n",
    "        # not same: 1\n",
    "        else:\n",
    "            flag = 1\n",
    "        return tf.cast(left[\"image\"], tf.float32) / 255., tf.cast(right[\"image\"], tf.float32) / 255., tf.cast(flag, tf.float32)\n",
    "\n",
    "    # initialize a dictionary to store all dataset\n",
    "    dataset = dict()\n",
    "\n",
    "    # calculate the number of pairs in training, test and validation sset\n",
    "    num_train = int(ds_info.splits['train'].num_examples / 2 * 0.8)\n",
    "    num_val = int(ds_info.splits['train'].num_examples / 2 * 0.2)\n",
    "    num_test = int(ds_info.splits['test'].num_examples / 2)\n",
    "\n",
    "    # split train set into train and validation\n",
    "    left_train = ds_train.take(num_train)\n",
    "    right_train = ds_train.skip(num_train).take(num_train)\n",
    "    ds_train = tf.data.Dataset.zip((left_train, right_train))\n",
    "\n",
    "    left_val = ds_train.skip(num_train + num_train).take(num_val)\n",
    "    right_val = ds_train.skip(num_train + num_train + num_val).take(num_val)\n",
    "    ds_val = tf.data.Dataset.zip((left_val, right_val))\n",
    "\n",
    "    left_test = ds_test.take(num_test)\n",
    "    right_test = ds_test.skip(num_test).take(num_test)\n",
    "    ds_test = tf.data.Dataset.zip((left_test, right_test))\n",
    "\n",
    "    # store left images, right images and flag to three numpy array\n",
    "    # training set\n",
    "    train_x_left = []\n",
    "    train_x_right = []\n",
    "    train_y = []\n",
    "\n",
    "    for left, right in ds_train:\n",
    "        left_x, right_x, flag = pairing(left, right)\n",
    "\n",
    "        train_x_left.append(left_x)\n",
    "        train_x_right.append(right_x)\n",
    "        train_y.append(flag)\n",
    "\n",
    "    train_x_left = np.array(train_x_left)\n",
    "    train_x_right = np.array(train_x_right)\n",
    "    train_y = np.array(train_y)\n",
    "\n",
    "    # store to dictionary\n",
    "    dataset[\"train_x_left\"] = train_x_left\n",
    "    dataset[\"train_x_right\"] = train_x_right\n",
    "    dataset[\"train_y\"] = train_y\n",
    "\n",
    "    # validation set \n",
    "    val_x_left = []\n",
    "    val_x_right = []\n",
    "    val_y = []\n",
    "\n",
    "    for left, right in ds_val:\n",
    "        left_x, right_x, flag = pairing(left, right)\n",
    "\n",
    "        val_x_left.append(left_x)\n",
    "        val_x_right.append(right_x)\n",
    "        val_y.append(flag)\n",
    "\n",
    "    val_x_left = np.array(val_x_left)\n",
    "    val_x_right = np.array(val_x_right)\n",
    "    val_y = np.array(val_y)\n",
    "\n",
    "    # store to dictionary\n",
    "    dataset[\"val_x_left\"] = val_x_left\n",
    "    dataset[\"val_x_right\"] = val_x_right\n",
    "    dataset[\"val_y\"] = val_y\n",
    "\n",
    "\n",
    "    # test set \n",
    "    test_x_left = []\n",
    "    test_x_right = []\n",
    "    test_y = []\n",
    "\n",
    "    for left, right in ds_test:\n",
    "        left_x, right_x, flag = pairing(left, right)\n",
    "\n",
    "        test_x_left.append(left_x)\n",
    "        test_x_right.append(right_x)\n",
    "        test_y.append(flag)\n",
    "\n",
    "    test_x_left = np.array(test_x_left)\n",
    "    test_x_right = np.array(test_x_right)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # store to dictionary\n",
    "    dataset[\"test_x_left\"] = test_x_left\n",
    "    dataset[\"test_x_right\"] = test_x_right\n",
    "    dataset[\"test_y\"] = test_y\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def base_model(input_shape):\n",
    "    \"\"\"The base branch for creating siamese networks. \n",
    "    A full siamese network is a combination of several base models.\"\"\"\n",
    "\n",
    "    # Layer 0: input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    # layer 1: Conv2D layer\n",
    "    x = layers.Conv2D(filters=64, kernel_size=(10,10), activation='relu')(input_layer)\n",
    "    # layer 2: MaxPool2D layer\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    # layer 3: Conv2D layer\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(7,7), activation='relu')(x)\n",
    "    # layer 4: MaxPool2D layer\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    # layer 5: Conv2D layer\n",
    "    x = layers.Conv2D(filters=128, kernel_size=(4,4), activation='relu')(x)\n",
    "    # layer 6: MaxPool2D layer\n",
    "    x = layers.MaxPool2D(pool_size=(2,2))(x)\n",
    "    # layer 7: Conv2D layer\n",
    "    x = layers.Conv2D(filters=256, kernel_size=(4,4), activation='relu')(x)\n",
    "    # Layer 8: flatten layer\n",
    "    x = layers.Flatten()(x)\n",
    "    # layer 9: Fully connected layer\n",
    "    x = layers.Dense(4096, activation='sigmoid')(x)\n",
    "\n",
    "    base_model = keras.Model(inputs=input_layer, outputs=x)\n",
    "    return base_model\n",
    "\n",
    "\n",
    "def contrastive_loss_network(input_shape):\n",
    "    \"\"\" Nested internal function\n",
    "        Based on two branches of base_model(), create siamese networks for usage of contrastive loss.\n",
    "\n",
    "    Returns:\n",
    "        contrastive_loss_model (tf.keras.Model): siamese networks for usage of contrastive loss\n",
    "    \"\"\"\n",
    "    model = base_model(input_shape)\n",
    "\n",
    "    # input layer for left and right branches\n",
    "    input_left = Input(input_shape)\n",
    "    input_right = Input(input_shape)\n",
    "\n",
    "    # init the left model and right model (two base_model())\n",
    "    left_model = model(input_left)\n",
    "    right_model = model(input_right)\n",
    "\n",
    "    contrastive_loss_model = keras.Model(inputs=[input_left, input_right], outputs=[left_model, right_model], name=\"contrastive_loss_network\")\n",
    "\n",
    "    return contrastive_loss_model\n",
    "\n",
    "\n",
    "\n",
    "def triplet_loss_network(input_shape):\n",
    "    \"\"\" Nested internal function\n",
    "        Based on three branches of base_model(), create siamese networks for usage of triplet loss.\n",
    "\n",
    "    Returns:\n",
    "        triplet_loss_model (tf.keras.Model): siamese networks for usage of triplet loss\n",
    "    \"\"\"\n",
    "    model = base_model(input_shape)\n",
    "\n",
    "    # input layer for anchor, positive and negative branches\n",
    "    input_anchor = Input(input_shape)\n",
    "    input_positive = Input(input_shape)\n",
    "    input_negative = Input(input_shape)\n",
    "\n",
    "    # init the anchor model, positive model and negative model (three base_model())\n",
    "    anchor_model = model(input_anchor)\n",
    "    positive_model = model(input_positive)\n",
    "    negative_model = model(input_negative)\n",
    "\n",
    "    triplet_loss_model = keras.Model(inputs=[input_anchor, input_positive, input_negative], outputs=[anchor_model, positive_model, negative_model], name=\"triplet_loss_network\")\n",
    "\n",
    "    return triplet_loss_model\n",
    "\n",
    "\n",
    "def loss(loss_name):\n",
    "    \"\"\"Implementation my own loss function for tensorflow.\n",
    "    Two loss functions are implemented here, \"contrastive_loss\" and \"triplet_loss\".\n",
    "\n",
    "    Args:\n",
    "        loss_name (str): the name of loss function. In this function, \n",
    "                        the name should be \"contrastive_loss\" or \"triplet_loss\".\n",
    "        margin (float, optional): m > 0 determines how far the embeddings of \n",
    "                        a negative pair should be pushed apart. Defaults to 1.\n",
    "    \"\"\"\n",
    " \n",
    "    def contrastive_loss(y_true, y_pred, margin = 1):\n",
    "        \"\"\"Implementation of the triplet loss function\n",
    "\n",
    "\n",
    "        Contrastive loss = 0.5 * mean( (1-true_value) * square(distance) + true_value * square( max(margin-distance, 0) ))\n",
    "\n",
    "        Args:\n",
    "            y_true (int): true label, positive pair (same class) -> 0, \n",
    "                        negative pair (different class) -> 1\n",
    "            \n",
    "            y_pred (list): python list containing two objects in a pair of tensors:\n",
    "                left : the encodings for one image data in a pair\n",
    "                right : the encodings for the other image data in a pair\n",
    "            margin (float, optional): m > 0 determines how far the embeddings of \n",
    "                        a negative pair should be pushed apart. Defaults to 1.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            loss (float): real number, value of the loss\n",
    "        \"\"\"\n",
    "\n",
    "        left = y_pred[0]\n",
    "        right = y_pred[1]\n",
    "\n",
    "        distance = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(left - right), axis=-1))\n",
    "\n",
    "        loss_positive = tf.math.square(distance)\n",
    "        loss_negative = tf.math.square(tf.maximum(0., margin - distance))\n",
    "        \n",
    "        loss = y_true * loss_negative + (1 - y_true) * loss_positive\n",
    "        loss = 0.5 * tf.math.reduce_mean(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def triplet_loss(y_true, y_pred, margin = 1):\n",
    "        \"\"\"Implementation of the triplet loss function\n",
    "\n",
    "        Arguments:\n",
    "            y_true : true labels, required when you define a loss in Keras, \n",
    "                    not applied in this function.\n",
    "\n",
    "            y_pred (list): python list containing three objects:\n",
    "                anchor : the encodings for the anchor data\n",
    "                positive : the encodings for the positive data (similar to anchor)\n",
    "                negative : the encodings for the negative data (different from anchor)\n",
    "            \n",
    "            margin (float, optional): m > 0 determines how far the embeddings of \n",
    "                        a negative data should be pushed apart. Defaults to 1.\n",
    "\n",
    "        Returns:\n",
    "            loss (float): real number, value of the loss\n",
    "        \"\"\"\n",
    "\n",
    "        anchor = y_pred[0]\n",
    "        positive = y_pred[1]\n",
    "        negative = y_pred[2]\n",
    "\n",
    "        # squared distance between the anchor and the positive\n",
    "        pos_dist = tf.math.reduce_sum(tf.math.square(anchor - positive), axis=-1)\n",
    "\n",
    "        # squared distance between the anchor and the negative\n",
    "        neg_dist = tf.math.reduce_sum(tf.math.square(anchor - negative), axis=-1)\n",
    "\n",
    "        # compute loss\n",
    "        basic_loss = margin + pos_dist - neg_dist\n",
    "        loss = tf.math.maximum(basic_loss,0.0)\n",
    "        loss = tf.math.reduce_mean(loss)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    if loss_name == 'contrastive_loss':\n",
    "        return contrastive_loss\n",
    "    \n",
    "    if loss_name == 'triplet_loss':\n",
    "        return triplet_loss\n",
    "\n",
    "\n",
    "def contrastive_loss_test():\n",
    "    \"\"\"Test implementation of contrastive loss function \n",
    "    code from week9 practice\"\"\"\n",
    "    num_data = 10\n",
    "    feat_dim = 6\n",
    "    margin = 0.2\n",
    "\n",
    "    embeddings = [np.random.rand(num_data, feat_dim).astype(np.float32),\n",
    "                np.random.rand(num_data, feat_dim).astype(np.float32)]\n",
    "    labels = np.random.randint(0, 1, size=(num_data)).astype(np.float32)\n",
    "\n",
    "    #Compute loss with numpy\n",
    "    loss_np = 0.\n",
    "    left = embeddings[0]\n",
    "    right = embeddings[1]\n",
    "\n",
    "    for i in range(num_data):\n",
    "        dist = np.sqrt(np.sum(np.square(left[i] - right[i])))\n",
    "        loss_pos = np.square(dist)\n",
    "        loss_neg = np.square(max(0. ,(margin - dist)))\n",
    "        loss_np += labels[i] * loss_neg + (1 - labels[i]) * loss_pos\n",
    "    loss_np /= num_data\n",
    "    loss_np *= 0.5\n",
    "    print('Contrastive loss computed with numpy', loss_np)\n",
    "\n",
    "    loss_tf = loss('contrastive_loss')\n",
    "\n",
    "    loss_tf_val = loss_tf(labels, embeddings, margin)\n",
    "    print('Contrastive loss computed with tensorflow', loss_tf_val)\n",
    "    assert np.allclose(loss_np, loss_tf_val)\n",
    "\n",
    "\n",
    "def triplet_loss_test():\n",
    "    \"\"\"Test if the triplet loss function works correctly\n",
    "    \"\"\"\n",
    "\n",
    "    #Test implementation of triplet loss function \n",
    "    # code from week9 practice\n",
    "    num_data = 10\n",
    "    feat_dim = 6\n",
    "    margin = 0.2\n",
    "\n",
    "    embeddings = [np.random.rand(num_data, feat_dim).astype(np.float32),\n",
    "                np.random.rand(num_data, feat_dim).astype(np.float32),\n",
    "                np.random.rand(num_data, feat_dim).astype(np.float32)]\n",
    "    labels = np.random.randint(0, 1, size=(num_data)).astype(np.float32)\n",
    "\n",
    "    #Compute loss with numpy\n",
    "    loss_np = 0.\n",
    "    anchor = embeddings[0]\n",
    "    positive = embeddings[1]\n",
    "    negative = embeddings[2]\n",
    "\n",
    "    for i in range(num_data):\n",
    "        pos_dist = np.sum(np.square(anchor[i] - positive[i]))\n",
    "        neg_dist = np.sum(np.square(anchor[i] - negative[i]))\n",
    "        loss_np += max(0. ,(margin + pos_dist - neg_dist))\n",
    "    loss_np /= num_data\n",
    "    print('Triplet loss computed with numpy', loss_np)\n",
    "\n",
    "    loss_tf = loss('triplet_loss')\n",
    "\n",
    "    loss_tf_val = loss_tf(labels, embeddings, margin)\n",
    "    print('Triplet loss computed with tensorflow', loss_tf_val)\n",
    "    assert np.allclose(loss_np, loss_tf_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "Start loading dataset - omniglot...\n",
      "\n",
      "Dataset  - omniglot loaded into train and test splits successfully.\n",
      "\n",
      "The list of all available labels for this dataset:\n",
      "['image', 'alphabet', 'alphabet_char_id', 'label']\n",
      "\n",
      "The input shape of the provided image in the dataset:\n",
      "(105, 105, 3)\n",
      "\n",
      "The number of images in the training set: 19280\n",
      "The number of images in the test set: 13180\n",
      "\n",
      "Preprocessing dataset for contrastive loss-based networks:\n",
      "It takes about 30 seconds...\n",
      "Function split_dataset_for_contrastive_networks finished\n",
      "===============================\n",
      "===============================\n",
      "Implement and test loss functions\n",
      "Triplet loss computed with numpy 0.23027804732322693\n",
      "Triplet loss computed with tensorflow tf.Tensor(0.23027804, shape=(), dtype=float32)\n",
      "Contrastive loss computed with numpy 0.4588031306862831\n",
      "Contrastive loss computed with tensorflow tf.Tensor(0.45880318, shape=(), dtype=float32)\n",
      "===============================\n",
      "===============================\n",
      "\n",
      "Build a base network for Siamese network\n",
      "\n",
      "\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 105, 105, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 96, 96, 64)        19264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 42, 42, 128)       401536    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 18, 128)       262272    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 256)         524544    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "=================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Build a Siamese network with contrastive loss\n",
      "\n",
      "\n",
      "Model: \"contrastive_loss_network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Functional)            (None, 4096)         38960448    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "\n",
      "Build a Siamese network with triplet loss\n",
      "\n",
      "\n",
      "Model: \"triplet_loss_network\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 105, 105, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Functional)            (None, 4096)         38960448    input_6[0][0]                    \n",
      "                                                                 input_7[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 38,960,448\n",
      "Trainable params: 38,960,448\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "===============================\n",
      "===============================\n",
      "\n",
      "Train the Siamese network with contrastive loss\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "31/31 [==============================] - 19s 223ms/step - loss: 0.0634 - model_1_loss: 0.0303 - model_1_1_loss: 0.0331\n",
      "Epoch 2/20\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.0438 - model_1_loss: 0.0216 - model_1_1_loss: 0.0221\n",
      "Epoch 3/20\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.0453 - model_1_loss: 0.0226 - model_1_1_loss: 0.0226\n",
      "Epoch 4/20\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.0451 - model_1_loss: 0.0230 - model_1_1_loss: 0.0222\n",
      "Epoch 5/20\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.0439 - model_1_loss: 0.0224 - model_1_1_loss: 0.0214\n",
      "Epoch 6/20\n",
      "31/31 [==============================] - 6s 194ms/step - loss: 0.0441 - model_1_loss: 0.0231 - model_1_1_loss: 0.0210\n",
      "Epoch 7/20\n",
      "31/31 [==============================] - 6s 205ms/step - loss: 0.0440 - model_1_loss: 0.0218 - model_1_1_loss: 0.0222\n",
      "Epoch 8/20\n",
      "31/31 [==============================] - 6s 200ms/step - loss: 0.0455 - model_1_loss: 0.0233 - model_1_1_loss: 0.0222\n",
      "Epoch 9/20\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.0477 - model_1_loss: 0.0230 - model_1_1_loss: 0.0247\n",
      "Epoch 10/20\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.0439 - model_1_loss: 0.0214 - model_1_1_loss: 0.0225\n",
      "Epoch 11/20\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.0439 - model_1_loss: 0.0214 - model_1_1_loss: 0.0225\n",
      "Epoch 12/20\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.0458 - model_1_loss: 0.0227 - model_1_1_loss: 0.0231\n",
      "Epoch 13/20\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.0449 - model_1_loss: 0.0227 - model_1_1_loss: 0.0222\n",
      "Epoch 14/20\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.0455 - model_1_loss: 0.0231 - model_1_1_loss: 0.0224\n",
      "Epoch 15/20\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.0437 - model_1_loss: 0.0217 - model_1_1_loss: 0.0220\n",
      "Epoch 16/20\n",
      "31/31 [==============================] - 6s 192ms/step - loss: 0.0446 - model_1_loss: 0.0217 - model_1_1_loss: 0.0229\n",
      "Epoch 17/20\n",
      "31/31 [==============================] - 6s 195ms/step - loss: 0.0438 - model_1_loss: 0.0223 - model_1_1_loss: 0.0215\n",
      "Epoch 18/20\n",
      "31/31 [==============================] - 6s 196ms/step - loss: 0.0451 - model_1_loss: 0.0218 - model_1_1_loss: 0.0233\n",
      "Epoch 19/20\n",
      "31/31 [==============================] - 6s 198ms/step - loss: 0.0457 - model_1_loss: 0.0236 - model_1_1_loss: 0.0222\n",
      "Epoch 20/20\n",
      "31/31 [==============================] - 6s 193ms/step - loss: 0.0440 - model_1_loss: 0.0216 - model_1_1_loss: 0.0224\n"
     ]
    }
   ],
   "source": [
    "# task: load and split omniglot dataset\n",
    "print('===============================')\n",
    "ds_train, ds_test, ds_info = load_dataset(\"omniglot\")\n",
    "print(\"Preprocessing dataset for contrastive loss-based networks:\")\n",
    "print(\"It takes about 30 seconds...\")\n",
    "dataset_contrastive_networks = split_dataset_for_contrastive_networks(ds_train, ds_test, ds_info, 'alphabet')\n",
    "print('Function split_dataset_for_contrastive_networks finished')\n",
    "\n",
    "print('===============================')\n",
    "\n",
    "# task: implement and test loss functions\n",
    "print('===============================')\n",
    "print('Implement and test loss functions')\n",
    "triplet_loss_test()\n",
    "contrastive_loss_test()\n",
    "print('===============================')\n",
    "\n",
    "# get the shape of the image\n",
    "input_shape = ds_info.features['image'].shape\n",
    "\n",
    "# task: build siamese network\n",
    "print('===============================')\n",
    "\n",
    "print('\\nBuild a base network for Siamese network\\n\\n')\n",
    "base = base_model(input_shape)\n",
    "base.summary()\n",
    "\n",
    "print('\\nBuild a Siamese network with contrastive loss\\n\\n')\n",
    "contrastive_loss_model = contrastive_loss_network(input_shape)\n",
    "contrastive_loss_model.summary()\n",
    "\n",
    "print('\\nBuild a Siamese network with triplet loss\\n\\n')\n",
    "triplet_loss_model = triplet_loss_network(input_shape)\n",
    "triplet_loss_model.summary()\n",
    "print('===============================')\n",
    "\n",
    "# task: train Siamese network\n",
    "print('===============================')\n",
    "\n",
    "print('\\nTrain the Siamese network with contrastive loss\\n\\n')\n",
    "contrastive_loss_model.compile(\n",
    "    loss=loss('contrastive_loss'),\n",
    "    optimizer=keras.optimizers.SGD(),\n",
    ")\n",
    "contrastive_loss_history = contrastive_loss_model.fit(\n",
    "    x=[dataset_contrastive_networks['train_x_left'], dataset_contrastive_networks['train_x_right']],\n",
    "    y=dataset_contrastive_networks['train_y'],\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    validation_data=([dataset_contrastive_networks['val_x_left'], dataset_contrastive_networks['val_x_right']], dataset_contrastive_networks['val_y']),\n",
    ")\n",
    "\n",
    "# print('\\nTrain the Siamese network with triplet loss\\n\\n')\n",
    "# triplet_loss_model.compile(\n",
    "#     loss=loss('contrastive_loss'),\n",
    "#     optimizer=keras.optimizers.SGD(),\n",
    "# )\n",
    "# triplet_loss_history = triplet_loss_model.fit(\n",
    "#     x=[dataset_contrastive_networks['train_x_left'], dataset_contrastive_networks['train_x_right']],\n",
    "#     y=dataset_contrastive_networks['train_y'],\n",
    "#     epochs=20,\n",
    "#     batch_size=256,\n",
    "#     validation_data=([dataset_contrastive_networks['val_x_left'], dataset_contrastive_networks['val_x_right']], dataset_contrastive_networks['val_y']),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206/206 [==============================] - 3s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "score = contrastive_loss_model.predict(x=[dataset_contrastive_networks['test_x_left'], dataset_contrastive_networks['test_x_right']], verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vec):\n",
    "    \"\"\"Find the Euclidean distance between two vectors.\n",
    "\n",
    "    Arguments:\n",
    "        vec: List containing two tensors of same length.\n",
    "\n",
    "    Returns:\n",
    "        Tensor containing euclidean distance\n",
    "        (as floating point value) between vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    x, y = vec\n",
    "    distance = tf.math.sqrt(tf.math.reduce_sum(tf.math.square(x - y), axis=-1, keepdims=True))\n",
    "    return distance\n",
    "\n",
    "def sigmoid(X):\n",
    "   return 1/(1+np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = sigmoid(euclidean_distance(score))\n",
    "\n",
    "threshold = 0.5\n",
    "correction = 0\n",
    "error = 0\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] >= 0.5:\n",
    "        pred = 1\n",
    "    else:\n",
    "        pred = 0\n",
    "    \n",
    "    if pred == dataset_contrastive_networks['test_y'][i]:\n",
    "        correction += 1\n",
    "    else:\n",
    "        error += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6232 358\n"
     ]
    }
   ],
   "source": [
    "print(correction,error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6590, 105, 105, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_contrastive_networks['test_x_left'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_contrastive_networks['val_x_left'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27752/92897117.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_contrastive_networks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_x_left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_contrastive_networks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_x_left'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 4 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "np.concatenate((dataset_contrastive_networks['train_x_left'], dataset_contrastive_networks['val_x_left']), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_contrastive_networks['val_y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = int(ds_info.splits['train'].num_examples / 2 * 0.8)\n",
    "num_val = int(ds_info.splits['train'].num_examples / 2 * 0.2)\n",
    "num_test = int(ds_info.splits['test'].num_examples / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7712 6590 1928\n"
     ]
    }
   ],
   "source": [
    "print(num_train, num_test, num_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_train, ds_test), ds_info = tfds.load(\"omniglot\", split=['train','test'], with_info=True, shuffle_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of pairs in training, test and validation sset\n",
    "num_train = int(ds_info.splits['train'].num_examples / 2 * 0.8)\n",
    "num_val = int(ds_info.splits['train'].num_examples / 2 * 0.2)\n",
    "num_test = int(ds_info.splits['test'].num_examples / 2)\n",
    "\n",
    "# split train set into train and validation\n",
    "left_train = ds_train.take(num_train)\n",
    "right_train = ds_train.skip(num_train).take(num_train)\n",
    "train = tf.data.Dataset.zip((left_train, right_train))\n",
    "\n",
    "left_val = ds_train.skip(num_train * 2).take(num_val)\n",
    "right_val = ds_train.skip(num_train * 2 + num_val).take(num_val)\n",
    "val = tf.data.Dataset.zip((left_val, right_val))\n",
    "\n",
    "left_test = ds_test.take(num_test)\n",
    "right_test = ds_test.skip(num_test).take(num_test)\n",
    "test = tf.data.Dataset.zip((left_test, right_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1928\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for _ in val:\n",
    "    count += 1\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5fbb69dc477f16c614d0279d3c32a08b8462b181ba1381a242aa9ddc9865207"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
